{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c91eec1b-84b1-41fc-866d-a8fadf7fa2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.2645166903863998\n",
      "Epoch: 1 Loss: 0.23005711934505102\n",
      "Epoch: 2 Loss: 0.197758300050919\n",
      "Epoch: 3 Loss: 0.1684826849349741\n",
      "Epoch: 4 Loss: 0.14264789869483074\n",
      "Epoch: 5 Loss: 0.12031026082185617\n",
      "Epoch: 6 Loss: 0.10128376283707374\n",
      "Epoch: 7 Loss: 0.08524914470528515\n",
      "Epoch: 8 Loss: 0.07183427343525045\n",
      "Epoch: 9 Loss: 0.060664975151627415\n",
      "Epoch: 10 Loss: 0.051392935947531596\n",
      "Epoch: 11 Loss: 0.04370832780992796\n",
      "Epoch: 12 Loss: 0.0373433027135264\n",
      "Epoch: 13 Loss: 0.03207054690630208\n",
      "Epoch: 14 Loss: 0.027699490119670517\n",
      "Epoch: 15 Loss: 0.024071662719319528\n",
      "Epoch: 16 Loss: 0.02105600141345032\n",
      "Epoch: 17 Loss: 0.018544494807030076\n",
      "Epoch: 18 Loss: 0.016448329658731137\n",
      "Epoch: 19 Loss: 0.014694575312247785\n",
      "Epoch: 20 Loss: 0.013223382023120608\n",
      "Epoch: 21 Loss: 0.011985641491395638\n",
      "Epoch: 22 Loss: 0.010941048959832778\n",
      "Epoch: 23 Loss: 0.01005650672917359\n",
      "Epoch: 24 Loss: 0.009304813866179928\n",
      "Epoch: 25 Loss: 0.00866359352312139\n",
      "Epoch: 26 Loss: 0.008114416231976701\n",
      "Epoch: 27 Loss: 0.007642084087395848\n",
      "Epoch: 28 Loss: 0.007234046588776688\n",
      "Epoch: 29 Loss: 0.006879923983275596\n",
      "Epoch: 30 Loss: 0.006571118254920863\n",
      "Epoch: 31 Loss: 0.006300495506985193\n",
      "Epoch: 32 Loss: 0.006062126471148642\n",
      "Epoch: 33 Loss: 0.0058510743362464104\n",
      "Epoch: 34 Loss: 0.005663221104676923\n",
      "Epoch: 35 Loss: 0.005495125330127394\n",
      "Epoch: 36 Loss: 0.0053439054305062725\n",
      "Epoch: 37 Loss: 0.0052071438595079275\n",
      "Epoch: 38 Loss: 0.005082808304839696\n",
      "Epoch: 39 Loss: 0.0049691867987778846\n",
      "Epoch: 40 Loss: 0.004864834208635177\n",
      "Epoch: 41 Loss: 0.004768528046514536\n",
      "Epoch: 42 Loss: 0.004679231920268409\n",
      "Epoch: 43 Loss: 0.004596065257853506\n",
      "Epoch: 44 Loss: 0.004518278189044669\n",
      "Epoch: 45 Loss: 0.004445230672900202\n",
      "Epoch: 46 Loss: 0.004376375125490328\n",
      "Epoch: 47 Loss: 0.0043112419374952\n",
      "Epoch: 48 Loss: 0.004249427381258698\n",
      "Epoch: 49 Loss: 0.004190583496502559\n",
      "Epoch: 50 Loss: 0.004134409617018699\n",
      "Epoch: 51 Loss: 0.004080645260370955\n",
      "Epoch: 52 Loss: 0.004029064151468663\n",
      "Epoch: 53 Loss: 0.00397946919085794\n",
      "Epoch: 54 Loss: 0.0039316882113509255\n",
      "Epoch: 55 Loss: 0.0038855703935277153\n",
      "Epoch: 56 Loss: 0.0038409832327611013\n",
      "Epoch: 57 Loss: 0.0037978099686261993\n",
      "Epoch: 58 Loss: 0.0037559474025659\n",
      "Epoch: 59 Loss: 0.003715304042074298\n",
      "Epoch: 60 Loss: 0.00367579851990206\n",
      "Epoch: 61 Loss: 0.003637358245269\n",
      "Epoch: 62 Loss: 0.0035999182510967342\n",
      "Epoch: 63 Loss: 0.0035634202071120266\n",
      "Epoch: 64 Loss: 0.0035278115735237134\n",
      "Epoch: 65 Loss: 0.0034930448740145698\n",
      "Epoch: 66 Loss: 0.003459077070160868\n",
      "Epoch: 67 Loss: 0.0034258690222023144\n",
      "Epoch: 68 Loss: 0.0033933850234397073\n",
      "Epoch: 69 Loss: 0.0033615923975051365\n",
      "Epoch: 70 Loss: 0.0033304611494030523\n",
      "Epoch: 71 Loss: 0.003299963662605751\n",
      "Epoch: 72 Loss: 0.0032700744356546213\n",
      "Epoch: 73 Loss: 0.0032407698527000817\n",
      "Epoch: 74 Loss: 0.003212027983240967\n",
      "Epoch: 75 Loss: 0.0031838284070244416\n",
      "Epoch: 76 Loss: 0.0031561520606581545\n",
      "Epoch: 77 Loss: 0.0031289811029871772\n",
      "Epoch: 78 Loss: 0.0031022987967134832\n",
      "Epoch: 79 Loss: 0.003076089404094866\n",
      "Epoch: 80 Loss: 0.0030503380948687688\n",
      "Epoch: 81 Loss: 0.003025030864804143\n",
      "Epoch: 82 Loss: 0.0030001544635102698\n",
      "Epoch: 83 Loss: 0.0029756963303189006\n",
      "Epoch: 84 Loss: 0.002951644537218759\n",
      "Epoch: 85 Loss: 0.002927987737960675\n",
      "Epoch: 86 Loss: 0.0029047151225697725\n",
      "Epoch: 87 Loss: 0.0028818163766040177\n",
      "Epoch: 88 Loss: 0.0028592816445850364\n",
      "Epoch: 89 Loss: 0.002837101497102635\n",
      "Epoch: 90 Loss: 0.0028152669011600913\n",
      "Epoch: 91 Loss: 0.0027937691933825094\n",
      "Epoch: 92 Loss: 0.0027726000557579053\n",
      "Epoch: 93 Loss: 0.0027517514936249363\n",
      "Epoch: 94 Loss: 0.002731215815654383\n",
      "Epoch: 95 Loss: 0.002710985615604889\n",
      "Epoch: 96 Loss: 0.00269105375565859\n",
      "Epoch: 97 Loss: 0.0026714133511686403\n",
      "Epoch: 98 Loss: 0.00265205775666778\n",
      "Epoch: 99 Loss: 0.0026329805530076363\n",
      "[0 0] actual: 0 predicted:0.0\n",
      "[0 1] actual: 1 predicted:1.0\n",
      "[1 0] actual: 1 predicted:1.0\n",
      "[1 1] actual: 0 predicted:0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    return (1/(1+np.exp(-x)))\n",
    "def diff_sigmoid(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "#load dataset\n",
    "X=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y=np.array([[0],[1],[1],[0]])\n",
    "\n",
    "#initialize weights and bias\n",
    "lr=0.1\n",
    "num_hidden_N=3\n",
    "epochs=100\n",
    "np.random.seed(10)\n",
    "W_hid=np.random.uniform(size=(X.shape[1],num_hidden_N))\n",
    "B_hid=np.random.uniform(size=(1,num_hidden_N))\n",
    "W_out=np.random.uniform(size=(num_hidden_N,Y.shape[1]))\n",
    "B_out=np.random.uniform(size=(1,Y.shape[1]))\n",
    "\n",
    "for i in range(epochs):\n",
    "#forward propagation\n",
    "    hid_in=np.dot(X,W_hid)+B_hid\n",
    "    hid_out=sigmoid(hid_in)\n",
    "    out_in=np.dot(hid_out,W_out)+B_out\n",
    "    out=sigmoid(out_in)\n",
    "    \n",
    "    #error calculation\n",
    "    Err=out-Y\n",
    "    hid_err=np.dot(Err,W_out.T) * diff_sigmoid(hid_in)\n",
    "    grad_W_out=np.dot(hid_out.T,Err)\n",
    "    grad_W_hid=np.dot(X.T,hid_err)\n",
    "\n",
    "    #update weights\n",
    "    W_out=W_out-lr*grad_W_out\n",
    "    B_out=B_out-lr*Err\n",
    "    W_hid=W_hid-lr*grad_W_hid\n",
    "    B_hid=B_hid-lr*hid_err\n",
    "    \n",
    "    print(f\"Epoch: {i} Loss: {np.mean(Err)}\")\n",
    "#prediction\n",
    "hid_in=np.dot(X,W_hid)+B_hid\n",
    "hid_out=sigmoid(hid_in)\n",
    "out_in=np.dot(hid_out,W_out)+B_out\n",
    "out=sigmoid(out_in)\n",
    "for i in range(X.shape[0]):\n",
    "    print(f\"{X[i]} actual: {Y[i][0]} predicted:{np.round(out[i][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2bebf9aa-5f3f-4106-b417-e33ba948134a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5167e1b-bf78-459d-850b-d7d9cd4b340e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1ed41b-5e1f-40ff-9903-654060ed75c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
